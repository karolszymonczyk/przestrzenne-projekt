{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import string\n",
    "import emoji\n",
    "import re\n",
    "import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import tokenize\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../../data/tweets_train.tsv', sep='\\t', converters={'target': str, 'id_str': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_model = KeyedVectors.load_word2vec_format('../../data/language_models/wiki.multi.pl.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "  url_free = re.compile(regex).sub('', text)\n",
    "  tokens = tokenize(url_free, lowercase=True)\n",
    "  return ' '.join(list(tokens))\n",
    "\n",
    "def filter_text(model, text):\n",
    "  return len([word for word in tokenize(text, lowercase=True) if word in model]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text'] = tweets.apply(lambda row: clean_text(row[\"full_text\"]), axis=1)\n",
    "tweets_found = tweets[tweets.apply(lambda row: filter_text(vector_model, row['clean_text']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(model, tweets, targets):\n",
    "  sequences = []\n",
    "  targets_final = []\n",
    "  for tweet, target in tqdm(zip(tweets, targets)):\n",
    "    words = []\n",
    "    for word in tokenize(tweet, lowercase=True):\n",
    "      if word in model:\n",
    "        vec = model.get_vector(word)\n",
    "        norm = np.linalg.norm(vec)\n",
    "        words.append(vec / norm)\n",
    "    if len(words) > 0:\n",
    "      sequences.append(torch.tensor(words))\n",
    "      targets_final.append(target)\n",
    "\n",
    "  max_len = max(sequences, key=lambda x: x.shape[0]).shape[0]\n",
    "  padded_sequences = []\n",
    "  for seq in sequences:\n",
    "    seq_len = seq.shape[0]\n",
    "    padding = torch.zeros(max_len - seq_len, 300)\n",
    "    padded_sequences.append(torch.cat((padding, seq), dim=0))\n",
    "\n",
    "  pad_text = torch.stack(padded_sequences)\n",
    "  labels_tensor = torch.tensor(targets_final)\n",
    "\n",
    "  return pad_text, labels_tensor\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7d996bf2d0660041\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7d996bf2d0660041\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class TweetsDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, tweets: pd.DataFrame, test_size: float=0.2, batch_size: int=128):\n",
    "        super().__init__()\n",
    "        self.tweets = tweets\n",
    "        self.test_size = test_size\n",
    "        self.batch_size = batch_size\n",
    "        self.le = LabelEncoder()\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        self.le.fit(tweets[\"target\"])\n",
    "        targets = tweets[\"target\"].apply(lambda x: self.le.transform([x])[0])\n",
    "        X, y = prepare(vector_model, tweets['clean_text'], targets)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.X_train, self.y_train), batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.X_test, self.y_test), batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.X_test, self.y_test), batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(TensorDataset(self.X_test, self.y_test), batch_size=self.batch_size)\n",
    "\n",
    "    def get_test_sets(self):\n",
    "        return self.X_test, self.y_test\n",
    "    \n",
    "    def get_label_encoder(self):\n",
    "        return self.le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class LSTM(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, bidirectional=False, learning_rate=1e-4):\n",
    "      super().__init__()\n",
    "      self.input_dim = input_dim  # this is the number of features\n",
    "      self.hidden_dim = hidden_dim\n",
    "      self.num_layers = 1\n",
    "      self.lstm = torch.nn.LSTM(input_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional)\n",
    "      self.classifier = nn.Sequential(\n",
    "        nn.Linear(in_features=2*hidden_dim if bidirectional else hidden_dim, out_features=output_dim),\n",
    "        nn.Softmax(dim=1)\n",
    "      )\n",
    "      self.learning_rate = learning_rate\n",
    "      self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      x: torch.Tensor\n",
    "  ) -> torch.Tensor:\n",
    "      out, (hn, _) = self.lstm(x)\n",
    "      return self.classifier(out[:, -1, :])\n",
    "\n",
    "      # return out\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten()\n",
    "    y_hat = self(x)\n",
    "    loss = self.loss(y_hat, y)\n",
    "    self.log('train_loss', loss, on_epoch=True, on_step=False)\n",
    "    return loss\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten()\n",
    "    y_hat = self(x)\n",
    "    loss = self.loss(y_hat, y)\n",
    "    y_hat = torch.argmax(self(x), 1)\n",
    "    f1 = f1_score(y.cpu(), y_hat.cpu(), average='micro')\n",
    "    acc = accuracy_score(y.cpu(), y_hat.cpu())\n",
    "    self.log(\"val_loss\", loss, prog_bar=True)\n",
    "    self.log(\"val_f1_micro\", f1, prog_bar=True)\n",
    "    self.log(\"val_acc\", acc, prog_bar=True)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten()\n",
    "    y_hat = self(x)\n",
    "    y_hat = torch.argmax(self(x), 1)\n",
    "\n",
    "    report = classification_report(y, y_hat, output_dict=True)\n",
    "    self.log_dict(report)\n",
    "    return report\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "  def predict_step(\n",
    "    self,\n",
    "    batch,\n",
    "    batch_idx: int,\n",
    "    dataloader_idx: Optional[int] = None,\n",
    "  ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    x, y = batch\n",
    "    z = self(x)\n",
    "    return z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "\n",
    "def train_model(dataloader, name, epochs=40, lr=1e-4, bidirectional=False):\n",
    "    AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "    model = LSTM(\n",
    "      input_dim=300,\n",
    "      hidden_dim=384,\n",
    "      output_dim=4,\n",
    "      learning_rate=lr,\n",
    "      bidirectional=bidirectional\n",
    "    )\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        dirpath=\"../../data/lstm/\",\n",
    "        filename=name,\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    tb_logger = pl_loggers.TensorBoardLogger(\"lightning_logs/\", name=name, log_graph=True)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        gpus=AVAIL_GPUS,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        logger=tb_logger,\n",
    "    )\n",
    "    trainer.fit(model, dataloader)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "2499it [00:01, 1417.87it/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "D:\\Users\\Michal\\Documents\\Studia\\semestr2\\cyfrowe\\cyfrowe\\lib\\site-packages\\pytorch_lightning\\loggers\\tensorboard.py:247: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | lstm       | LSTM             | 2.1 M \n",
      "1 | classifier | Sequential       | 3.1 K \n",
      "2 | loss       | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 M     Total params\n",
      "8.442     Total estimated model params size (MB)\n",
      "D:\\Users\\Michal\\Documents\\Studia\\semestr2\\cyfrowe\\cyfrowe\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:631: UserWarning: Checkpoint directory D:\\Users\\Michal\\Documents\\Studia\\semestr2\\cyfrowe\\project-proposal-pyka-szymonczyk-warzyniak\\data\\lstm exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Michal\\Documents\\Studia\\semestr2\\cyfrowe\\cyfrowe\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Michal\\Documents\\Studia\\semestr2\\cyfrowe\\cyfrowe\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 79/79 [00:01<00:00, 57.45it/s, loss=0.929, v_num=3, val_loss=1.270, val_f1_micro=0.464, val_acc=0.464]\n"
     ]
    }
   ],
   "source": [
    "data = TweetsDataModule(tweets=tweets, batch_size=32)\n",
    "trainer = train_model(data, 'lstm_384', epochs=50, bidirectional=True, lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3891a413c9413807c1dd9657114be1b483746a632775f2fb4ed6f769f0aee0d8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
