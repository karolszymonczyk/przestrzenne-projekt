{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_json('../../data/embedded_tweets.jl', lines=True, dtype=False)\n",
    "df_train = pd.read_json('../../data/embedded_train_150.jl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train[\"target\"] == \"NA\", [\"target\"]] = 0  # added\n",
    "\n",
    "df_train['target'] = df_train['target'].astype(str)\n",
    "df_train = df_train.rename(columns={'id':'id_str'})\n",
    "df_train['id_str'] = df_train['id_str'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(n_components=213)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "split_df = pd.DataFrame(df_train[\"embedding\"].tolist())\n",
    "df_train = pd.concat([split_df, df_train], axis=1)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train[\"target\"])\n",
    "df_train[\"target\"] = df_train[\"target\"].apply(lambda x: le.transform([x])[0])\n",
    "\n",
    "additional = [\"id_str\", \"created_at\", \"full_text\", \"embedding\", \"target\"]\n",
    "x = df_train.drop(additional, axis=1).values\n",
    "y = df_train.loc[:, [\"target\"]].values\n",
    "\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "pca = PCA(213)\n",
    "pca.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df = pd.DataFrame(df_tweets[\"embedding\"].tolist())\n",
    "temp_df = pd.concat([split_df, df_tweets], axis=1)\n",
    "\n",
    "additional = [\"id_str\", \"created_at\", \"download_datetime\", \"full_text\", \"favorite_count\", \"in_reply_to_screen_name\", \"lang\", \"quote_count\", \"reply_count\", \"retweet_count\", \"user_id_str\", \"embedding\"]\n",
    "\n",
    "x = temp_df.drop(additional, axis=1).values\n",
    "# x = pca.transform(x)\n",
    "\n",
    "df_tweets[\"embedding\"] = pd.DataFrame(x).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from typing import Optional, Tuple\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "  \n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, lr=1e-4):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.lr = lr\n",
    "    \n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      # nn.Linear(hidden_dim, hidden_dim),\n",
    "      # nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, out_features=output_dim),\n",
    "      nn.Softmax(dim=1)\n",
    "    )\n",
    "    self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "  \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten().type(torch.LongTensor)\n",
    "    y_hat = self(x)\n",
    "    loss = self.loss(y_hat, y)\n",
    "    self.log('train_loss', loss, on_epoch=True, on_step=False)\n",
    "    return {\"loss\": loss, \"logits\": y_hat.detach(), \"gold\": y, \"batch_idx\": batch_idx}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten().type(torch.LongTensor)\n",
    "    y_hat = self(x)\n",
    "    loss = self.loss(y_hat, y)\n",
    "    y_hat = torch.argmax(self(x), 1)\n",
    "    f1 = f1_score(y.cpu(), y_hat.cpu(), average='micro')\n",
    "    acc = accuracy_score(y.cpu(), y_hat.cpu())\n",
    "    self.log(\"val_loss\", loss, prog_bar=True)\n",
    "    self.log(\"val_f1_micro\", f1, prog_bar=True)\n",
    "    self.log(\"val_acc\", acc, prog_bar=True)\n",
    "    return loss\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y = y.flatten().type(torch.LongTensor)\n",
    "    y_hat = self(x)\n",
    "    y_hat = torch.argmax(self(x), 1)\n",
    "\n",
    "    report = classification_report(y, y_hat, output_dict=True)\n",
    "    self.log_dict(report)\n",
    "    return report\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "      batch_size = self.trainer.datamodule.get_batch_size()\n",
    "      sample_map = self.trainer.datamodule.get_sample_map()\n",
    "\n",
    "      data = {\"guid\": [], f\"logits_epoch_{self.current_epoch}\": [], \"gold\": []}\n",
    "      for batch in outputs:\n",
    "        batch_idx = batch[\"batch_idx\"]\n",
    "        curr_batch_size = len(batch[\"logits\"])\n",
    "        data[\"guid\"] += [sample_map[batch_size*batch_idx + idx] for idx in range(curr_batch_size)]\n",
    "        data[f\"logits_epoch_{self.current_epoch}\"] += batch[\"logits\"].tolist()\n",
    "        data[\"gold\"] += batch[\"gold\"].tolist()\n",
    "\n",
    "      df = pd.DataFrame(data)\n",
    "      df.to_json(f\"./training_dynamics/dynamics_epoch_{self.current_epoch}.jsonl\", lines=True, orient='records')\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "  def predict_step(\n",
    "    self,\n",
    "    batch,\n",
    "    batch_idx: int,\n",
    "    dataloader_idx: Optional[int] = None,\n",
    "  ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    x, y = batch\n",
    "    z = self(x)\n",
    "    return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLP.load_from_checkpoint(checkpoint_path=f'../../data/mlp/mlp_newdata_3c_pca_213_213_bs=32_lr-4.ckpt', input_dim=213, hidden_dim=213, output_dim=3)\n",
    "mlp = MLP.load_from_checkpoint(checkpoint_path=f'../../data/mlp/test_no_pca.ckpt', input_dim=768, hidden_dim=768, output_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, le):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    return le.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_predict(x):\n",
    "    xt = torch.tensor(x).unsqueeze(0)\n",
    "    return predict(mlp, xt, le)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92668/92668 [07:39<00:00, 201.53it/s] \n"
     ]
    }
   ],
   "source": [
    "df_tweets[\"target\"] = df_tweets[\"embedding\"].progress_apply(label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.drop(\"embedding\", inplace=True, axis=1)\n",
    "# df_tweets.to_csv(\"../../data/classified_tweets.tsv\", sep=\"\\t\")\n",
    "df_tweets.to_csv(\"../../data/classified_tweets_no_pca.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     60302\n",
       "-1    17417\n",
       "0     14949\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af91805feb772e5ca358e0d7e8c046b76a3384a8e0392a1e50dac8eea804f488"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
